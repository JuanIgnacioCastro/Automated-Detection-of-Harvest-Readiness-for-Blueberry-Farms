# -*- coding: utf-8 -*-
"""Copy of HarvestReadiness05.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DCwJWUw7GuCNBZ0ZFHkkhjH7hmsx6pjA
"""

from google.colab import drive

drive.mount('/content/gdrive')

!pip install ultralytics

import torch

if torch.cuda.is_available():
    print("âœ… GPU is available! Using:", torch.cuda.get_device_name(0))
else:
    print("âŒ GPU not available. Using CPU.")

import os
import random
from glob import glob
from collections import defaultdict
from math import ceil

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from ultralytics import YOLO

ROOT_DIR = "/content/gdrive/MyDrive/CODE4"
RUN_DIR  = ROOT_DIR + "/runs/detect/train"
MODEL_PATH = RUN_DIR + "/weights/best.pt"

# ============================================
# 1) LOAD TRAINING HYPERPARAMETERS FROM RUN
# ============================================
import os
import yaml
import pandas as pd

ROOT_DIR = "/content/gdrive/MyDrive/CODE4"
RUN_DIR  = os.path.join(ROOT_DIR, "runs", "detect", "train")

# Try to find the config file that stores training hyperparameters
cand_files = ["args.yaml", "opt.yaml", "hyp.yaml"]
cfg_path = None
for fname in cand_files:
    path = os.path.join(RUN_DIR, fname)
    if os.path.exists(path):
        cfg_path = path
        break

if cfg_path is None:
    raise FileNotFoundError(
        f"Could not find any of {cand_files} in {RUN_DIR}. "
        "Open the run folder in Drive and check the exact yaml name."
    )

print(f"âœ… Using hyperparameter file: {cfg_path}")

with open(cfg_path, "r") as f:
    cfg = yaml.safe_load(f)

# Pick the hyperparameters you care about for the slide
important_keys = [
    # basic training setup
    "model", "data", "epochs", "batch", "imgsz", "device", "optimizer", "patience",
    # learning rate schedule
    "lr0", "lrf", "momentum", "weight_decay", "warmup_epochs",
    # loss weights
    "box", "cls", "dfl",
    # augmentation
    "hsv_h", "hsv_s", "hsv_v",
    "degrees", "translate", "scale", "shear",
    "flipud", "fliplr", "mosaic", "mixup"
]

hyperparams = {k: cfg.get(k, None) for k in important_keys if k in cfg}

hyperparams_df = (
    pd.DataFrame.from_dict(hyperparams, orient="index", columns=["value"])
    .sort_index()
)

print("\n=== TRAINING HYPERPARAMETERS SUMMARY ===")
display(hyperparams_df)

# ============================================
# 2) EVALUATE MODEL ON TEST SPLIT (UPDATED)
# ============================================
import os
import glob
import torch
import pandas as pd
import numpy as np
from ultralytics import YOLO

# --- Paths ---
ROOT_DIR = "/content/gdrive/MyDrive/CODE4"
RUN_DIR  = os.path.join(ROOT_DIR, "runs", "detect", "train")
MODEL_PATH = os.path.join(RUN_DIR, "weights", "best.pt")

# Your test images folder
TEST_IMAGES_DIR = os.path.join(ROOT_DIR, "data02 - test", "images")

# Your .yaml config file (the one you mentioned)
DATA_YAML = "/content/gdrive/MyDrive/CODE4/google_colab_config.yaml"

# --- Quick sanity check: test images exist? ---
test_images = sorted(
    glob.glob(os.path.join(TEST_IMAGES_DIR, "*.jpg")) +
    glob.glob(os.path.join(TEST_IMAGES_DIR, "*.jpeg")) +
    glob.glob(os.path.join(TEST_IMAGES_DIR, "*.png")) +
    glob.glob(os.path.join(TEST_IMAGES_DIR, "*.bmp"))
)

print(f"Found {len(test_images)} test images in: {TEST_IMAGES_DIR}")
if len(test_images) == 0:
    raise FileNotFoundError(
        f"No images found in {TEST_IMAGES_DIR}. "
        "Check that your test set is correctly placed there."
    )

# --- Load model ---
device = 0 if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

model = YOLO(MODEL_PATH)

# Use training cfg if available, otherwise fall back to defaults
imgsz = 640
batch = 16
if "cfg" in globals() and isinstance(cfg, dict):
    imgsz = cfg.get("imgsz", imgsz)
    batch = cfg.get("batch", batch)

print(f"val() with imgsz={imgsz}, batch={batch}")

# IMPORTANT:
# google_colab_config.yaml must point its 'data' or 'test' sections
# to the same dataset that contains TEST_IMAGES_DIR.
metrics = model.val(
    data=DATA_YAML,       # your yaml file
    split="test",         # evaluate on test split defined in the yaml
    imgsz=imgsz,
    batch=batch,
    device=device,
    plots=True,           # saves confusion matrix, PR curves, etc.
    save_json=True        # COCO-style JSON if you need it
)

print("\n=== RAW METRICS OBJECT ===")
print(metrics)

# ============================================
# 2a) OVERALL TEST SET METRICS
# ============================================
metrics_dict = metrics.results_dict  # YOLOv8 Metrics â†’ dict

summary = {
    "mAP_50_95": metrics_dict.get("metrics/mAP50-95(B)", None),
    "mAP_50":    metrics_dict.get("metrics/mAP50(B)", None),
    "precision": metrics_dict.get("metrics/precision(B)", None),
    "recall":    metrics_dict.get("metrics/recall(B)", None)
}

summary_df = pd.DataFrame([summary])
print("\n=== OVERALL TEST SET METRICS ===")
display(summary_df)

# ============================================
# 2b) PER-CLASS mAP (GREEN vs RIPE)
# ============================================
# metrics.box.maps is an array: [mAP_class0, mAP_class1, ...]
per_class_map = np.array(metrics.box.maps)

# get class names from the model (e.g. {0: 'green', 1: 'ripe'})
class_names = model.names

per_class_df = pd.DataFrame({
    "class_id": list(class_names.keys()),
    "class_name": list(class_names.values()),
    "mAP_50_95": per_class_map
})

print("\n=== PER-CLASS mAP@0.5:0.95 ON TEST SET ===")
display(per_class_df)







# =======================
# 1) CONFIG
# =======================

# Class IDs (make sure these match your model's class order)
UNRIPE_CLASS_ID = 0
RIPE_CLASS_ID   = 1

# Plant selection
SELECTED_PLANTS = None   # e.g., ["Plant1","Plant2"]; None => take first NUM_PLANTS (sorted)
NUM_PLANTS      = 10

# Paths
ROOT_DIR = '/content/gdrive/MyDrive/CODE4'
model_path = os.path.join(ROOT_DIR, 'runs/detect/train/weights/best.pt')
test_images_dir = os.path.join(ROOT_DIR, 'data02 - test/images')

# Synthetic generation / acres
N_SYNTH          = 12_000     # total plants to generate
TOTAL_NOISE_PCT  = 0.30       # ~Â±30% variation in total berries (multiplicative)
KAPPA            = 50         # Beta concentration (higher => ratio closer to base)
RANDOM_SEED      = 42
PLANTS_PER_ACRE  = 1200       # â†’ 10 acres when N_SYNTH=12,000

# =======================
# 2) LOAD MODEL & IMAGES
# =======================

# Collect test images
image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')
test_images = [p for p in glob(os.path.join(test_images_dir, '*'))
               if os.path.splitext(p)[1].lower() in image_extensions]

print(f"âœ… Found {len(test_images)} test images.")
if not test_images:
    raise ValueError("âŒ No test images found. Check test_images_dir and extensions.")

# Load YOLO model
model = YOLO(model_path)

# =======================
# 3) GROUP IMAGES BY PLANT
# =======================

grouped_images = defaultdict(list)
for p in test_images:
    pid = os.path.basename(p).split('_')[0]   # e.g., "Plant5" from "Plant5_151.jpg"
    grouped_images[pid].append(p)

plant_ids = sorted(grouped_images.keys())
if SELECTED_PLANTS is not None:
    wanted = set(SELECTED_PLANTS)
    plant_ids = [p for p in plant_ids if p in wanted]
else:
    plant_ids = plant_ids[:NUM_PLANTS]

print(f"\nðŸ”Ž Evaluating plants: {', '.join(plant_ids)}")
for pid in plant_ids:
    print(f"  - {pid}: {len(grouped_images[pid])} image(s)")

# =======================
# 4) PER-PLANT DETECTION â†’ DF (10 plants)
# =======================

def count_ripe_unripe(img_path):
    r = model(img_path, verbose=False)[0]  # suppress per-image logs
    if r.boxes is None or r.boxes.cls is None or len(r.boxes.cls) == 0:
        return 0, 0
    cls_ids = r.boxes.cls.int().tolist()
    u = sum(1 for c in cls_ids if c == UNRIPE_CLASS_ID)
    ri = sum(1 for c in cls_ids if c == RIPE_CLASS_ID)
    return u, ri

results = []
batch_unripe = 0
batch_ripe = 0

for pid in plant_ids:
    plant_u = 0
    plant_r = 0
    for ipath in grouped_images[pid]:
        u, r = count_ripe_unripe(ipath)
        plant_u += u
        plant_r += r

    total = plant_u + plant_r
    ripe_ratio = (plant_r / total) if total > 0 else 0.0

    results.append({
        "Plant": pid,
        "Ripe": plant_r,
        "Unripe": plant_u,
        "RipeRatio": round(ripe_ratio, 2)
    })

    batch_unripe += plant_u
    batch_ripe += plant_r

df = pd.DataFrame(results)

# Optional batch row (kept for display; we don't use it to generate synthetics)
batch_total = batch_unripe + batch_ripe
batch_ripe_ratio = (batch_ripe / batch_total) if batch_total > 0 else 0.0
df.loc[len(df)] = {
    "Plant": "BATCH_TOTAL",
    "Ripe": batch_ripe,
    "Unripe": batch_unripe,
    "RipeRatio": round(batch_ripe_ratio, 2)
}

print("\nâœ… Base 10-plant DF created:")
display(df)

# =======================
# 5) GENERATE 12,000 UNIQUE PLANTS (NO DUPLICATES)
#     - Totals vary (multiplicative noise)
#     - Ripe ratio varies around base (Beta), then clamped to base range
#     - Ripe drawn Binomial so ripe+unripe == total (integers)
# =======================

base = df.loc[df["Plant"] != "BATCH_TOTAL", ["Plant", "Ripe", "Unripe"]].reset_index(drop=True).copy()
assert len(base) == 10, "Expected 10 base plants after filtering."

# derive original ratio range from base (or hard-set to 0.34â€“0.60 if you prefer)
base_totals = (base["Ripe"] + base["Unripe"]).to_numpy()
base_ratios = np.divide(base["Ripe"], base_totals, out=np.zeros_like(base_totals, dtype=float), where=base_totals>0)
RIPE_RATIO_MIN = float(np.min(base_ratios))
RIPE_RATIO_MAX = float(np.max(base_ratios))
# To force exactly [0.34, 0.60], uncomment:
# RIPE_RATIO_MIN, RIPE_RATIO_MAX = 0.34, 0.60

rng = np.random.default_rng(RANDOM_SEED)
rows = []
for i in range(N_SYNTH):
    b = base.iloc[rng.integers(0, len(base))]
    b_ripe = int(b["Ripe"])
    b_unripe = int(b["Unripe"])
    b_total = b_ripe + b_unripe
    b_ratio = (b_ripe / b_total) if b_total > 0 else 0.0

    # new total
    total_factor = 1.0 + rng.normal(0, TOTAL_NOISE_PCT)
    new_total = max(0, int(round(b_total * total_factor)))

    # new ratio centered on base ratio (Beta), then clamp to observed range
    eps = 1e-6
    alpha = max(b_ratio * KAPPA, eps)
    beta  = max((1.0 - b_ratio) * KAPPA, eps)
    new_ratio = float(rng.beta(alpha, beta)) if (alpha > 0 and beta > 0) else 0.0
    new_ratio = min(max(new_ratio, RIPE_RATIO_MIN), RIPE_RATIO_MAX)

    # sample ripe/unripe
    new_ripe = int(rng.binomial(n=new_total, p=new_ratio)) if new_total > 0 else 0
    new_unripe = new_total - new_ripe

    rows.append({
        "BasePlant": b["Plant"],
        "Ripe": new_ripe,
        "Unripe": new_unripe
    })

df_12k = pd.DataFrame(rows)
df_12k["RipeRatio"] = (df_12k["Ripe"] / (df_12k["Ripe"] + df_12k["Unripe"])).fillna(0.0).round(2)
df_12k.insert(0, "PlantID", [f"P{i+1:05d}" for i in range(len(df_12k))])

# shuffle once (critical) and assign acres
df_12k = df_12k.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)
df_12k.insert(0, "Acre", (df_12k.index // PLANTS_PER_ACRE) + 1)  # 10 acres * 1200 plants

print("\nâœ… Synthetic 12k DF (shuffled & acres assigned):")
display(df_12k.head(10))

# Acre summary (should NOT be identical across acres)
acre_summary = (
    df_12k.groupby("Acre")[["Ripe", "Unripe"]]
    .sum()
    .assign(RipeRatio=lambda d: d["Ripe"] / (d["Ripe"] + d["Unripe"]))
    .round(3)
)
print("\n===== Acre-level summary (randomized) =====")
display(acre_summary)

# Quick variability check
print("\nStd of counts (should be > 0):")
print(df_12k[["Ripe","Unripe"]].std())

# # Save your dataframe to Excel
# df_12k.to_excel("df_12k.xlsx", index=False)

# # Download it
# from google.colab import files
# files.download("df_12k.xlsx")

def acre_manual_viability(
    acre_df,
    ripe_col="Ripe",
    unripe_col="Unripe",
    acre_col="Acre",
    **cfg,  # ðŸ‘ˆ pass any override here, e.g. avg_weight_per_berry_lb=0.005
):
    """
    Evaluate manual-harvest profitability per acre.

    Usage examples:
    acre_manual_viability(acre_summary)  # defaults
    acre_manual_viability(acre_summary, hourly_rate=18, workers=60)
    acre_manual_viability(acre_summary, pct_6oz=0.15, pct_12oz=0.65, pct_18oz=0.20, price_12oz=4.75)
    acre_manual_viability(acre_summary, trays_per_hour_per_worker=2.5, tray_capacity_lbs=5.5)
    """

    # ---- defaults (can be overridden via **cfg) ----
    DEFAULTS = dict(
        avg_weight_per_berry_lb=0.0045,   # ~2g
        tray_capacity_lbs=6.0,
        trays_per_hour_per_worker=3.0,
        workers=50,
        workday_hours=8.0,
        hourly_rate=16.0,
        bus_daily_cost=200.0,
        bus_capacity=40,                   # workers per bus
        housing_per_day_per_worker=15.0,
        food_water_per_day_per_worker=10.0,
        pct_6oz=0.20, pct_12oz=0.60, pct_18oz=0.20,
        price_6oz=3.00, price_12oz=5.00, price_18oz=7.00,
        use_ripe_only_for_weight=True      # set False to use (Ripe+Unripe)
    )
    p = {**DEFAULTS, **cfg}

    # ---- basic validation / normalization ----
    # ensure required cols exist
    req = [acre_col, ripe_col, unripe_col]
    df = acre_df.copy()
    if acre_col not in df.columns:
        df = df.reset_index()
        assert acre_col in df.columns, f"'{acre_col}' column not found after reset_index()."

    for c in [ripe_col, unripe_col]:
        assert c in df.columns, f"Missing column: {c}"

    # normalize pack percentages to sum to 1.0 (if user changed them)
    pct_sum = p["pct_6oz"] + p["pct_12oz"] + p["pct_18oz"]
    if pct_sum <= 0:
        raise ValueError("pct_6oz + pct_12oz + pct_18oz must be > 0")
    p["pct_6oz"]  /= pct_sum
    p["pct_12oz"] /= pct_sum
    p["pct_18oz"] /= pct_sum

    # ---- weight from berries ----
    if p["use_ripe_only_for_weight"]:
        berries_for_weight = df[ripe_col].astype(float)
    else:
        berries_for_weight = (df[ripe_col] + df[unripe_col]).astype(float)

    df["TotalWeight_lbs"] = berries_for_weight * float(p["avg_weight_per_berry_lb"])

    # ---- trays & labor (fractional trays) ----
    df["EffectiveTrays"] = df["TotalWeight_lbs"] / float(p["tray_capacity_lbs"])
    total_tray_rate = float(p["trays_per_hour_per_worker"]) * int(p["workers"])  # trays/h crew
    df["Hours_AllWorkers"] = np.where(total_tray_rate > 0, df["EffectiveTrays"] / total_tray_rate, 0.0)
    df["Days_AllWorkers"]  = np.where(float(p["workday_hours"]) > 0, df["Hours_AllWorkers"] / float(p["workday_hours"]), 0.0)

    # ---- costs ----
    df["PickerHours"]   = df["Hours_AllWorkers"] * int(p["workers"])
    df["PickerWages"]   = df["PickerHours"] * float(p["hourly_rate"])
    days_ceiled         = np.ceil(df["Days_AllWorkers"]).astype(int)
    buses_per_day       = int(np.ceil(int(p["workers"]) / float(p["bus_capacity"]))) if (p["workers"] > 0 and p["bus_capacity"] > 0) else 0
    df["TransportCost"] = days_ceiled * buses_per_day * float(p["bus_daily_cost"])
    df["HousingCost"]   = days_ceiled * float(p["housing_per_day_per_worker"]) * int(p["workers"])
    df["FoodWaterCost"] = days_ceiled * float(p["food_water_per_day_per_worker"]) * int(p["workers"])
    df["TotalCost"]     = df["PickerWages"] + df["TransportCost"] + df["HousingCost"] + df["FoodWaterCost"]

    # ---- packaging & revenue ----
    w6, w12, w18 = 0.375, 0.75, 1.125
    df["W6_lbs"]  = df["TotalWeight_lbs"] * p["pct_6oz"]
    df["W12_lbs"] = df["TotalWeight_lbs"] * p["pct_12oz"]
    df["W18_lbs"] = df["TotalWeight_lbs"] * p["pct_18oz"]

    df["Boxes_6oz"]  = (df["W6_lbs"]  // w6).astype(int)
    df["Boxes_12oz"] = (df["W12_lbs"] // w12).astype(int)
    df["Boxes_18oz"] = (df["W18_lbs"] // w18).astype(int)

    df["Rev_6oz"]  = df["Boxes_6oz"]  * float(p["price_6oz"])
    df["Rev_12oz"] = df["Boxes_12oz"] * float(p["price_12oz"])
    df["Rev_18oz"] = df["Boxes_18oz"] * float(p["price_18oz"])
    df["Revenue"]  = df["Rev_6oz"] + df["Rev_12oz"] + df["Rev_18oz"]

    # ---- profit & recommendation ----
    df["Profit"]        = df["Revenue"] - df["TotalCost"]
    df["IsProfitable"]  = df["Profit"] > 0
    df["Recommendation"] = np.where(df["IsProfitable"], "âœ… Harvest manually", "âŒ Do NOT harvest")

    # ---- pretty outputs ----
    round_cols = [
        "TotalWeight_lbs","EffectiveTrays","Hours_AllWorkers","Days_AllWorkers",
        "PickerHours","PickerWages","TransportCost","HousingCost","FoodWaterCost",
        "TotalCost","Revenue","Profit"
    ]
    for c in round_cols:
        df[c] = df[c].astype(float).round(2)

    # Reorder for readability
    cols_front = [acre_col, ripe_col, unripe_col,
                  "TotalWeight_lbs","EffectiveTrays",
                  "Hours_AllWorkers","Days_AllWorkers","PickerHours",
                  "PickerWages","TransportCost","HousingCost","FoodWaterCost",
                  "TotalCost","Boxes_6oz","Boxes_12oz","Boxes_18oz",
                  "Revenue","Profit","IsProfitable","Recommendation"]
    remaining = [c for c in df.columns if c not in cols_front]
    return df[cols_front + remaining]

# 1) Just defaults
acre_eval = acre_manual_viability(acre_summary)
acre_eval

# # 2) Change wages and crew size
# acre_eval = acre_manual_viability(acre_summary, hourly_rate=18, workers=60)
# acre_eval

# # 3) Change pack mix and prices
# acre_eval = acre_manual_viability(acre_summary, pct_6oz=0.15, pct_12oz=0.65, pct_18oz=0.20, price_12oz=4.75)
# acre_eval

# # 4) Use (Ripe + Unripe) to compute weight (instead of ripe only)
# acre_eval = acre_manual_viability(acre_summary, use_ripe_only_for_weight=False)
# acre_eval

import numpy as np
import pandas as pd

def acre_pick_simple(
    acre_df,
    ripe_col="Ripe",
    unripe_col="Unripe",
    acre_col="Acre",
    *,
    avg_weight_per_berry_lb=0.0045,   # ~2g
    tray_capacity_lbs=6.0,
    trays_per_hour_per_worker=3.0,
    workers=50,
    use_ripe_only_for_weight=True,
):
    """
    Minimal calculator:
      - TotalWeight_lbs
      - EffectiveTrays (fractional)
      - Hours_AllWorkers to finish with X workers
    """

    df = acre_df.copy()

    # Ensure the acre column exists (works with index-as-acre too)
    if acre_col not in df.columns:
        df = df.reset_index()
        if acre_col not in df.columns:
            raise ValueError(f"'{acre_col}' column not found, even after reset_index().")

    for c in [ripe_col, unripe_col]:
        if c not in df.columns:
            raise ValueError(f"Missing column: {c}")

    # Choose which berries count toward weight
    berries_for_weight = df[ripe_col].astype(float) if use_ripe_only_for_weight \
                         else (df[ripe_col] + df[unripe_col]).astype(float)

    # Weight and trays
    df["TotalWeight_lbs"] = berries_for_weight * float(avg_weight_per_berry_lb)
    df["EffectiveTrays"]  = df["TotalWeight_lbs"] / float(tray_capacity_lbs)

    # Crew throughput (trays/hour)
    total_tray_rate = float(trays_per_hour_per_worker) * int(workers)
    df["Hours_AllWorkers"] = np.where(total_tray_rate > 0,
                                      df["EffectiveTrays"] / total_tray_rate,
                                      0.0)

    # Pretty rounding
    for c in ["TotalWeight_lbs", "EffectiveTrays", "Hours_AllWorkers"]:
        df[c] = df[c].astype(float).round(2)

    return df[[acre_col, ripe_col, unripe_col,
               "TotalWeight_lbs", "EffectiveTrays", "Hours_AllWorkers"]]

acre_pick_simple(acre_summary)  # defaults
# acre_pick_simple(acre_summary, workers=60, trays_per_hour_per_worker=2.5)
# acre_pick_simple(acre_summary, use_ripe_only_for_weight=False, tray_capacity_lbs=5.5)















